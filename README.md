### Projet : Pipeline d‚ÄôIngestion & Mod√©lisation CitiBike + Weather

(Snowflake + dbt)

# Objectif

Ce projet met en place un pipeline complet de data engineering permettant d‚Äôing√©rer, nettoyer, transformer et mod√©liser :

des donn√©es de trajets de v√©los (CitiBike),

des donn√©es m√©t√©o journali√®res issues de fichiers JSON.

L‚Äôobjectif est de reproduire une architecture moderne orient√©e ingestion automatis√©e, mod√©lisation analytique et orchestration, sans se concentrer sur la visualisation.

# üß± Architecture globale

L'architecture suit une structure en plusieurs couches :

- Sources brutes :

    fichiers JSON m√©t√©o stock√©s dans un bucket S3 (stage externe),

    fichiers CSV CitiBike (stage interne).

- Zones d‚Äôingestion Snowflake :

    un stage externe pour la m√©t√©o,

    un stage interne pour les donn√©es CitiBike.

- Tables RAW :

    une table pour la m√©t√©o,

    une table pour les trajets.

- Mod√©lisation dbt :

    mod√®les de staging,

    dimensions (date, station),

    table de faits (trajets),

    mod√®le final combinant trajets + m√©t√©o dominante du jour.

- Orchestration :

    t√¢ches Snowflake programm√©es pour l‚Äôingestion,

    job dbt ex√©cut√© automatiquement chaque jour √† 1h.

# Ingestion des donn√©es m√©t√©o

Les donn√©es m√©t√©o proviennent d‚Äôun bucket S3 contenant des fichiers JSON, chacun repr√©sentant une journ√©e compl√®te de donn√©es √† New York.

Un stage externe Snowflake permet d‚Äôacc√©der directement √† ces fichiers.
Une t√¢che Snowflake quotidienne, ex√©cut√©e √† minuit, lit les fichiers JSON et les charge dans la table RAW d√©di√©e.

# Ingestion des donn√©es CitiBike

Les fichiers CSV CitiBike sont charg√©s dans un stage interne Snowflake depuis la machine locale.
Ils sont ensuite ing√©r√©s dans la table RAW via une t√¢che Snowflake ex√©cut√©e chaque heure.

Cette approche permet un chargement progressif et continu des donn√©es.

# D√©claration des sources dbt

Les deux tables RAW (bikes et weather) sont d√©clar√©es comme ¬´ sources ¬ª dans dbt.
Cela permet de :

    structurer la documentation,

    renforcer le lineage,

    activer des tests de qualit√©,

    am√©liorer la lisibilit√© du pipeline.

# √âtape de Staging dbt

La couche staging standardise les donn√©es :

    nettoyage des fichiers CSV CitiBike (suppression des guillemets parasites et des lignes invalides),

    normalisation des colonnes,

    pr√©paration des donn√©es pour la mod√©lisation.

Le staging agit comme un tampon propre entre les tables brutes et les mod√®les analytiques.

# Macros dbt

Deux macros ont √©t√© cr√©√©es pour enrichir les donn√©es temporelles :

    d√©termination de la saison √† partir d‚Äôune date,

    classification des jours en week-end ou jour ouvr√©.

Ces transformations √©vitent la duplication de logique dans les mod√®les.

# Dimension Date

La dimension date fournit des informations enrichies telles que :

    la date,

    l‚Äôheure,

    la saison,

    le type de jour,

    et d‚Äôautres attributs utiles aux analyses.

Elle permet d‚Äôanalyser les trajets selon le temps, l'heure, la saisonnalit√© ou les cycles hebdomadaires.

# Dimension Station

La dimension station extrait les m√©tadonn√©es des stations :

    identifiant,

    nom,

    position g√©ographique.

Elle constitue une table de r√©f√©rence unique et propre.

# Table de faits : trajets

La table de faits rassemble les mesures cl√©s li√©es aux trajets :

    date du trajet,

    station de d√©part et station d'arriv√©e,

    type d‚Äôutilisateur,

    dur√©e du trajet, calcul√©e √† partir des timestamps.

Elle est directement nourrie par le mod√®le de staging.

# Mod√©lisation m√©t√©o journali√®re

Les donn√©es m√©t√©o initialement horaires sont agr√©g√©es pour produire une table journali√®re.
Cette table contient :

    la m√©t√©o dominante du jour,

    les moyennes de temp√©rature, de pression, de couverture nuageuse et d'humidit√©.

Cette vue simplifi√©e facilite le croisement avec les trajets.

# Mod√®le final : trajets + m√©t√©o

Le mod√®le final combine :

    la table de faits des trajets,

    la m√©t√©o journali√®re.

R√©sultat : une table analytique enrichie, permettant d‚Äô√©tudier les comportements d‚Äôutilisation du service de v√©los en fonction des conditions m√©t√©orologiques.

# Orchestration du pipeline

Le pipeline fonctionne automatiquement gr√¢ce √† :

    une t√¢che Snowflake quotidienne pour l‚Äôingestion m√©t√©o,

    une t√¢che Snowflake horaire pour l‚Äôingestion des trajets,

    un job dbt horaire pour ex√©cuter l‚Äôensemble des mod√®les.

Cette orchestration assure une mise √† jour continue et fiable des donn√©es.